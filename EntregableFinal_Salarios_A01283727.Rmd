---
title: "Entregable Final: Salarios"
author: "Russel Rosique"
output:
  pdf_document: default
  html_document: default
date: "2022-08-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


#EL PROBLEMA

Identifica las condiciones que hacen que una persona especialista en analizar datos tenga un mejor sueldo de acuerdo con la base de datos que proporciona Kaggle en una muestra de personas que se dedican al analisis de datos en diferentes partes del mundo. 

```{r}
M=read.csv("ds_salaries.csv")
```

#DESCRIPCION DE LOS DATOS
| **Columna**              | **Descripción**                                                                                                                                                                                    |
|--------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| X1              | Número de identificación                  |
| X2              | Nombre del lago                           |
| X3              | Alcalinidad                               |
| X4              | PH                                        |
| X5              | Calcio (mg/l)                             |
| X6              | Clorofila (mg/l)                          |
| X7              | concentración media de mercurio (parte por millón) en el tejido muscualar del grupo de peces estudiados en cada lago   
                                                              |
| X8              | Número de peces estudiados en el lago                                                                                      |
| X9          | mínimo de la concentración de mercurio en cada grupo de peces
                                                              |
| X10 | máximo de la concentración de mercurio en cada grupo de peces                                                                          |
| X11     | estimación (mediante regresión) de la concentración de mercurio en el pez de 3 años (o promedio de mercurio cuando la edad no está disponible)                                                                       |
| X12     | indicador de la edad de los peces (0: jóvenes; 1: maduros)                                                                         |

#EXPLORACION VARIABLES

Primero que nada determinaremos los datos que no son relevantes al analisis que queremos usar. 

El primero que utilizaremos es WORK_YEAR, este es muy relevante ya que podemos ver que tanto han incrementado o declinado los salarios durante el tiempo. 
```{r}
work_year = M$work_year 
table(work_year) #NUMERICA DISCRETA
```

Otro que es altamente relevante es el titulo o puesto.
```{r}
job_title = M$job_title
table(job_title) #CUALITATIVA NOMINAL
```

Se puede observar que existen muchos puestos en el conjunto de datos, por lo que sería muy complejo e inefectivo utilizar todas las columnas para el análisis. Una solución a esto es encontrar una forma de agrupación, o bien, eliminar los puestos en donde solo se tiene un dato.

Por otro lado, tenemos otra variable relevante para nuestra investigación que es el tipo de empleo, es decir, tiempo completo, medio tiempo, entre otras opciones. 
```{r}
employment_type = M$employment_type
table(employment_type) #CUALITATIVA NOMINAL
```
```{r}
remote_ratio = M$remote_ratio
table(remote_ratio) #CUALITATIVA NOMINAL
```

El mas evidente son las columnas de SALARIO + DIVISA, para esta informacion usaremos SALARY_IN_USD ya que nos brindara una unidad estandarizada entre todos los salarios. Si utilizaramos los de SALARIO + DIVISA tendriamos unidades en diferentes DIVISAS lo cual no seria bueno para nuestro analisis. Nos trae un estandar para el salario.

```{r}
#EN VEZ DE UTILIZAR ESTO
#salary = M$salary
#salary_currency = M$salary_currency

#UTILIZAREMOS SOLO ESTE
salary_in_usd = M$salary_in_usd #CUANTITATIVA DISCRETA

print(paste('Range ',max(salary_in_usd)-min(salary_in_usd),' / ', 'Variance ',var(salary_in_usd),' / ', 'STD ',sd(salary_in_usd)))
summary(salary_in_usd)

```


Se analiza que la ubicación de la empresa también es relevante hacia el salario de los científicos de datos, sin embargo, al igual que con la variable de puestos, tenemos muchos países que cuentan con un solo dato por lo que se tendrá que hacer algo al respecto para tener un mejor análisis. Una buena solución a este problema sería únicamente seleccionar los países con más datos y trabajar con ellos, o bien, agrupar los datos en continentes. 

```{r}
company_location = M$company_location
table(company_location) #CUALITATIVA NOMINAL
```
```{r}
employee_residence = M$employee_residence
table(employee_residence) #CUALITATIVA NOMINAL
```


El tamaño de la empresa también nes relevante para el salario. Esta también se tendrá que convertir a variable dummy.
```{r}
company_size = M$company_size
table(company_size) #CUALITATIVA ORDINAL
```

Problemas de datos:
Primero checamos los NA en cada columna

```{r}
colSums(is.na(M))
```
Viendo que no tenemos NAs y ya viendo las frecuencias de todos los valores en las columnas, vemos que no hay indiscripencias en lo datos y podemos proceder.

#EXPLORACION

Primero que nada, buscaremos encontrar algunos datos generales sobre nuestra base de datos.

Haremos algunas pruebas de normalidad para los salarios (en USD), esto es solo con las datos que tenemos sin limpiar o filtrar por alguna categoria.

```{r}
hist(salary_in_usd, col='steelblue', main='Histograma Salarios',breaks=70)
```
El histograma (incluso si quitaramos los datos atipicos) muestra un sesgo hacia la izquierda, lo cual no nos indica una distribucion normal


#QQ Plot Salarios
```{r}
qqnorm(salary_in_usd, main='QQ Plot Salarios')
qqline(salary_in_usd)
```
Confirmamos la información previa del histograma, que existe un sesgo hacia la izquierda, por lo que no hay normalidad en los datos.

#Sesgo
Para encontrar el valor exacto del sesgo, se utiliza la siguiente libreria.
```{r}
library(e1071)
skewness(salary_in_usd)
```
El sesgo es muy grande positivo, lo cual indica que se inclina a la derecha, que es lo que se muestra igualmente en nuestro QQplot e Histograma.

En conclusion todas nuestras pruebas indican que no es una distribucion normal.

#PREPARACION DE LOS DATOS

Prepararemos los datos alrededor de nuestra variable objetivo: los salarios (especificamente los salarios en usd, debido a lo que explicamos anteriormente). 

Dado que anteriormente se pudo ver en el histograma la existencia de datos atípicos, se debe realizar una limpieza de dichos valores. Primero que nada, se utiliza un boxplot para definir de forma certera los datos atípicos, y posteriormente eliminarlos para disminuir el sesgo del modelo. 

```{r}
boxplot(salary_in_usd, horizontal = TRUE)
```

Se muestran bastantes datos atípicos en el diagrama, por lo que se decide remover estos datos del conjunto de datos para tener un análisis más preciso sin datos irrelevantes.  

Primero determinaremos los rangos intercuartiles
```{r}
Q <- quantile(salary_in_usd, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(salary_in_usd)

#Utilizamos 1.5 para datos atipicos + extremos, se utiliza 3 para solo quitar los datos extremos, 
up <-  Q[2]+1.5*iqr # Upper Range  
low <- Q[1]-1.5*iqr # Lower Range
```

Ya con los limites que tenemos, actualizaremos la matriz con la nueva informacion
```{r}
#Respaldamos una matriz con outliers
matrixConOutliers <- M
M<- subset(M, (M$salary_in_usd > low) & (M$salary_in_usd < up))
salary_in_usd <- M$salary_in_usd
```

#VOLVER A CHECAR DISTRIBUCION DE X
```{r}
hist(salary_in_usd, col='steelblue', main='Histograma Salarios',breaks=70)
```

El histograma nos muestra que sigue sin ser una distribucion normal pero como tenemos muchos datos no nos importa la distribucion de x ya que $\bar{x}$ sera distribuida normalmente

Asi quedo mucho mejor, nos da un mejor "scope" en nuestra informacion. Los datos atipicos solamente alterarian nuestro analisis.


#DUMMY VARIABLES
Como se menciona en el análisis de las variables, es necesario cambiar algunas de las columnas a variables dummy. Esto se hace de la siguiente manera:

```{r}
M$contract <- ifelse(M$employment_type == 'CT', 1, 0)
M$freelance <- ifelse(M$employment_type == 'FL', 1, 0)
M$fullTime <- ifelse(M$employment_type == 'FT', 1, 0)
M$partTime <- ifelse(M$employment_type == 'PT', 1, 0)

M$entry <- ifelse(M$experience_level == 'EN', 1, 0)
M$midlevel <- ifelse(M$experience_level == 'MI', 1, 0)
M$senior <- ifelse(M$experience_level == 'SE', 1, 0)
M$executive <- ifelse(M$experience_level == 'EX', 1, 0)

M$fullRemote <- ifelse(M$remote_ratio == 100, 1, 0)
M$partialRemote <- ifelse(M$remote_ratio == 50, 1, 0)
M$noRemote <- ifelse(M$remote_ratio == 0, 1, 0)

M$largeCompany <- ifelse(M$company_size == 'L', 1, 0)
M$mediumCompany <- ifelse(M$company_size == 'M', 1, 0)
M$smallCompany <- ifelse(M$company_size == 'S', 1, 0)
```

Para nuestra finalidad, como nuestra variable objetivo es el salario, podemos agrupar por estas variables sin tener que hacer las dummies, sin embargo, si en un futuro deseamos hacer algun tipo de modelo de regresion lineal o algo parecido, seran necesitadas las dummy variables.


#INTERVALOS DE CONFIANZA

A continuacion haremos algunas funciones para calcular los intervalos de confianza de acuerdo a nuestra funcion
```{r}
getError <- function(X, alfa) {
  n = length(X)
  media = mean(X) #Igual podriamos utilizar una media hipotesis como miu, en este caso usaremos la misma que la muestra
  sigma = sd(X)
  
  ErrorEst = sigma/sqrt(n)
  E = abs(qnorm(alfa/2))*ErrorEst
  return(E)
}
  
printLimits <- function(X, alfa) {
  E <- getError(X, alfa)
  media <- mean(X)
    
  lowerLimit =  media- E
  upperLimit = media + E
  
  print(paste("Intervalo de ", floor((1-alfa)*100),"% de confianza:"))
  print(paste("Lower Limit: ", floor(lowerLimit)))
  print(paste("Average: ", floor(media)))
  print(paste("Upper Limit: ", floor(upperLimit)))
  return(c(lowerLimit,media,upperLimit))
}
```
```{r}
plotInt <- function(maxIntervals, main_, labels_, xmin, xmax) {
  plot(0, ylim=c(0,maxIntervals+1), xlim=c(xmin-100,xmax+100),yaxt="n", ylab="", main=main_)
  axis(2, at=c(1:maxIntervals), labels=labels_)
}
addInt <- function(info, n) {
  #Para insertar los intervalos [Ai,Bi], con Ai y Bi son los límites inferior y superior del intervalo i respectivamente:
  arrows(info[1], n, info[3], n, angle=90, code=3, length = 0.1, lwd = 2)
  #Para dibujar la media dentro del  intervalo (insertar un punto), donde mi es la media del intervalo i:
  points(info[2], n, pch=19, cex=1.1)
}
```

Primero con nuestros salarios sin categorizar y un alpha de 0.01 (99% de confianza)
```{r}
res <- printLimits(salary_in_usd, 0.01)
plotInt(1, "CI for Salaries, 99% confidence" ,c("Salarios en USD"), min(res),max(res))
addInt(res, 1)
```
Esto es un buen "fit" ya que podemos estar 99% seguros que la media de la poblacion esta entre  y  lo cual es un rango muy pequeno.

#ANALISIS DE DATOS

Buscaremos tener un vistazo a los salarios de acuerdo con diferentes variables categoricas.

Para esto crearemos una función que nos de las estadisticas a partir de un subset de los datos:
```{r}
getStats <- function(subset, title) {
  print(paste(title,':'))
  print(paste('N datos: ',length(subset),' / ','Range ',max(currYearSalary)-min(subset),' / ', 'Variance ',var(subset),' / ', 'STD ',sd(subset)))
  print(summary(subset))
  writeLines("\n")
  printLimits(subset, 0.01)
  writeLines("\n\n")
}
```


#SUMMARY DE SALARIO POR AÑO
Encontraremos la media y la desviacion estandar por año para los salarios
```{r}
X<-split(M, M$work_year)
years = names(X)

plotInt(length(years), "CI for Salaries by Year, 99% confidence" ,c(years), min(salary_in_usd),max(salary_in_usd))
counter <- 1
for (year in years){
  currYear<-X[[year]]
  currYearSalary = currYear$salary_in_usd
  getStats(currYearSalary,paste('Year ',year))
  
  res <- printLimits(currYearSalary, 0.01)
  addInt(res, counter)
  counter <- counter + 1
}
```
A partir de estos intervalos de confianza, se logra observar con 99% de seguridad, la media de los salarios progresivamente a través de los años. Dicho esto, se puede ver que del 2020 al 2021 no existe un cambio suficientemente significativo, sin embargo, para el 2022, el intervalo de confianza se encuentra completamente mayor a los años anteriores con un 99% de confianza, lo que indica un claro incremento en el salario del 2022. La cantidad de datos es la misma con todos los años, lo que indica que tenemos aproximadamente el mismo nivel de precisión en nuestros datos. Esto nos muestra que la demanada para analistas de datos está subiendo. 

#SUMMARY DE SALARIO POR TAMAÑO DE LA EMPRESA
Se considera que un analisis pertinente podria ser el salario en base a la empresa, ya que se sabe que existen empresas mejor pagadas que otras.

Se hace lo mismo que en el apartado anterior, pero se cambian los años por el tamaño de la empresa.
```{r}
X<-split(M, M$company_size)
sizes = names(X)

plotInt(length(sizes), "CI for Salaries by Company Size, 99% confidence" ,c(sizes), min(salary_in_usd),max(salary_in_usd))
counter <- 1

for (size in sizes){
  currSize<-X[[size]]
  currSizeSalary = currSize$salary_in_usd
  getStats(currSizeSalary,paste('Size ',size))
  
  res <- printLimits(currSizeSalary, 0.01)
  addInt(res, counter)
  counter <- counter + 1
}
```
Se puede observar algo similar a lo obtenido con los años. En este caso, no podemos obtener una clara distinción entre las empresas medianas y grandes, ya que los intervalos de sus medias se empalman. Sin embargo, vemos una clara distinción con el salario en las empresas pequeñas. Se logra observar que el salario en empresas pequeñas está con 99% de confianza por debajo de las empresas medianas y grandes, dato que no es sorprendente ya que es evidente que las empresas pequeñas no tendrán las mismas ganancias que las empresas medianas o grandes.  

Primero que nada para ajustar con estos ejecutivos, hay que utilizar nuestro "dataset" con todos los datos (incluyendo atipicos)
```{r}
dividedBySize<-split(matrixConOutliers, matrixConOutliers$company_size)
summary(dividedBySize$`L`$salary_in_usd)
```

Como podemos ver ya con los datos atipicos incluidos, el promedio es mucho mas alto que las empresas medianas y pequenas lo cual indica que mueve mucho la media incluir todos estos puestos ejecutivos. Ahora analizaremos esta misma informacion (company_size = L) pero divido por nivel de experiencia.

```{r}
X<-split(dividedBySize$`L`, dividedBySize$`L`$experience_level)
niveles = names(X)

plotInt(length(niveles), "CI for Salaries Large Company, by XP Level, 99% confidence" ,c(niveles), min(dividedBySize$`L`$salary_in_usd),max(dividedBySize$`L`$salary_in_usd))
counter <- 1

for (nivelExperiencia in niveles){
  currExperiencia<-X[[nivelExperiencia]]
  currXPSalary = currExperiencia$salary_in_usd
  getStats(currXPSalary,paste('Nivel de Experiencia en Empresa Grande: ',nivelExperiencia))
  
  res <- printLimits(currXPSalary, 0.01)
  addInt(res, counter)
  counter <- counter + 1
}


```
Se cumple lo esperado, el puesto de ejecutivo es el claro ganador ya que cuenta con muchos datos muy elevados, al borde de lo atípico. Se puede analizar que EN y MI son igual, pero Ejecutivo y Senior no solo son mas altos si no que mucho mas altos. Y observamos mucha variacion en cuanto al intervalo de ejecutivo. 

```{r}
X<-split(dividedBySize$`M`, dividedBySize$`M`$experience_level)
niveles = names(X)

#Usaremos los mismos xlims que en la empresa grande para observar la diferencia
plotInt(length(niveles), "CI for Salaries Medium Company, by XP Level, 99% confidence" ,c(niveles), min(dividedBySize$`L`$salary_in_usd),max(dividedBySize$`L`$salary_in_usd))
counter <- 1

for (nivelExperiencia in niveles){
  currExperiencia<-X[[nivelExperiencia]]
  currXPSalary = currExperiencia$salary_in_usd
  getStats(currXPSalary,paste('Nivel de Experiencia en Empresa Grande: ',nivelExperiencia))
  
  res <- printLimits(currXPSalary, 0.01)
  addInt(res, counter)
  counter <- counter + 1
}

```
Al hacer lo mismo para las empresas medianas, se obtiene un resultado con gran similitud, por lo que se concluye que no influye el tamaño de la empresa, para el salario en los distintos puestos para un científico de datos.


#SALARIO POR PAIS

El siguiente análisis por realizar es el de salario por país, sin embargo, como se menciona en la exploración de las variables, se tienen muchos países con muy pocos datos. Por ello, se decidió tomar a los países que tuvieran un mínimo de 30 datos. 

```{r}
X<-split(M, M$company_location)

#Check all which have more than 30 data points
minDatos = 30
paises = names(X)
paisesFiltrados <- vector()

#Filtramos los paises
for (pais in paises){
  currPais<-X[[pais]]
  currPSalary = currPais$salary_in_usd
  if (length(currPSalary) >= minDatos) {
    paisesFiltrados <- c(paisesFiltrados, pais)
  }
}

plotInt(length(paisesFiltrados), "CI for Salaries by Country, 99% confidence" ,c(paisesFiltrados), min(M$salary_in_usd),max(M$salary_in_usd))
counter <- 1

for (paisFiltrado in paisesFiltrados){
  currPais<-X[[paisFiltrado]]
  currPSalary = currPais$salary_in_usd

  getStats(currPSalary,paste('Pais ',pais))
  
  res <- printLimits(currPSalary, 0.01)
  addInt(res, counter)
  counter <- counter + 1
  
}
```
Finalmente los países restantes fueron Canadá, Reino Unido y Estados Unidos. Se eliminaron la mayoría de los países, sin embargo, es evidente que los que quedaron son donde más oferta de trabajo hay, o más bien, donde más demanda por científicos de datos existe. Se realizó una comparativa de sus intervalos de confianza, con la cual se obtuvo un resultado claro, que la media de los salarios en Estados Unidos, es con 99% de confianza, más alta que en Canadá y Reino Unido. El análisis podría terminar aquí, sin embargo, se analiza que para saber con seguridad que país es mejor para trabajar, también se debería de realizar el cálculo combinado a la cantidad de impuestos por país.

#CONCLUSION

se obtuvieron diversos resultados muy interesantes con el análisis realizado. Entre los descubrimientos más destacados fue la influencia del tamaño de la empresa en los salarios, así como el incremento de salarios por año en la industria de las ciencias de datos. Se puede concluir que esta industria está en constante crecimiento debido a la era en la que vivimos y en la importancia que tiene la información en la actualidad, y que además de esto, existen muchas oportunidades para científicos de datos, ya sea en empresas pequeñas, medianas o grandes, alrededor de todo el mundo. 